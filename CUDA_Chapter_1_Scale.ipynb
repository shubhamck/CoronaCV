{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7mDZdK8QQTKFZSuvU7phN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubhamck/CoronaCV/blob/master/CUDA_Chapter_1_Scale.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to Cuda C++ Tutorials in Google Colab\n",
        "This tutorial series will start from scratch and will assume no knowledge about CUDA.\n",
        "\n",
        "## Chapter 1 : Running your first C++ Cuda program in Google Colab\n",
        "\n",
        "### Setting up Colab with Nvidia GPU\n",
        "As you all know Google colab is a very useful tool to prototype some cool Python or Machine Learning model using Python. But in the background a Google colab notebook can be assumed to be just another linux computer.\n",
        "\n",
        "When you open a new Colab Notebook, Google gives a very minimal setup with a simple CPU with no GPU.\n",
        "\n",
        "So first lets tell Google to give us a GPU:\n",
        "\n",
        "* Click on `Runtime` in the above Toolbar\n",
        "* Click on `Change Runtime` and Select `T4 GPU`\n",
        "![image.png](https://i.postimg.cc/VsZnBTFv/colab-gpu.png)\n",
        "\n",
        "So Now Lets test if Google gave us our GPU. Run `nvidia-smi`"
      ],
      "metadata": {
        "id": "gr66uKopSNMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!which nvcc\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Irv9BhK_DqN",
        "outputId": "85d9b930-0d6f-4109-c416-9932903284f7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/cuda/bin/nvcc\n",
            "Sat Aug 10 23:08:31 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> # â›” Alert\n",
        "> Google only allows free GPU access for a limited time.\n",
        "> I suggest `Change Runtime` back to `CPU` while coding or if you keep the `GPU` ON for a long time Google will ask you to pay to cotinue using it"
      ],
      "metadata": {
        "id": "Q5s11tSEXFo2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Writing a simple cuda kernel to multiply a number to all elements in an array on the GPU\n",
        "\n",
        "The first CUDA kernel that everyone learns to write a simple scalar multiplcation operation on a large array\n",
        "\n",
        "Lets look at the steps to do this:\n",
        "\n",
        "1. Create an array on the CPU\n",
        "2. Initialize the array with some `ints`\n",
        "3. Create a pointer called `device pointer` which will point to the memory location on the GPU where the input array will be copied\n",
        "4. Copy the input array to the GPU\n",
        "5. Run the Kernel\n",
        "6. Copy the the result array back to the CPU\n",
        "7. Verify if the kernel performed the action as expected"
      ],
      "metadata": {
        "id": "FJlI81AoZCGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scale.cu\n",
        "\n",
        "#include<stdio.h>\n",
        "#include <iostream>\n",
        "\n",
        "// Kernel which runs on every thread\n",
        "__global__ void scale(int* in, int* out, int scale_factor)\n",
        "{\n",
        "    // Each thread knows its ID\n",
        "\n",
        "    int id = threadIdx.x;\n",
        "\n",
        "    // Using the thread ID we query the input array, multiply it by the scale factor and copy to result\n",
        "    // to the output array using the same ID\n",
        "\n",
        "    out[id] = in[id] * scale_factor;\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc,char **argv)\n",
        "{\n",
        "    // Define some some constants\n",
        "    constexpr size_t ARRAY_SIZE = 64; // Size of input array\n",
        "    constexpr size_t ARRAY_BYTES = ARRAY_SIZE * sizeof(int); // Size of input array in bytes\n",
        "    constexpr int SCALE_FACTOR = 5; // scale factor\n",
        "\n",
        "    // Initialize input array on CPU\n",
        "    int h_in[ARRAY_SIZE];\n",
        "    for (int i = 0; i < ARRAY_SIZE; ++i)\n",
        "    {\n",
        "      h_in[i] = i;\n",
        "    }\n",
        "\n",
        "    // Declare output array on CPU\n",
        "    int h_out[ARRAY_SIZE];\n",
        "\n",
        "    // Declare pointer on CPU which point to memory locations on GPU global memory\n",
        "    int* d_in;\n",
        "    int* d_out;\n",
        "\n",
        "    // Allocate memory on the GPU\n",
        "    cudaMalloc((void**) &d_in, ARRAY_BYTES);\n",
        "    cudaMalloc((void**) &d_out, ARRAY_BYTES);\n",
        "\n",
        "    // Copy input array from CPU to GPU\n",
        "    cudaMemcpy(d_in, h_in, ARRAY_BYTES, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Run the Kernel\n",
        "    scale<<<1, ARRAY_SIZE>>>(d_in, d_out, SCALE_FACTOR);\n",
        "\n",
        "    // Copy output array from GPU to CPU\n",
        "    cudaMemcpy(h_out, d_out, ARRAY_BYTES, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Free the memory on the GPU\n",
        "    cudaFree(d_in);\n",
        "    cudaFree(d_out);\n",
        "\n",
        "    // Flush\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "\n",
        "    // Print output array on the CPU\n",
        "    for (int i = 0; i < ARRAY_SIZE; ++i)\n",
        "    {\n",
        "      // printf(\"%d\\n\", h_out[i]);\n",
        "      std::cout << i << \" : \" << h_out[i] << \"\\n\";\n",
        "    }\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsPNLrNp_MYS",
        "outputId": "c53d397e-a728-44da-82a2-e776de3e3054"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scale.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets compile the code."
      ],
      "metadata": {
        "id": "PMPdZi6Pi37S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /usr/local/cuda\n",
        "!ln -s /usr/local/cuda-12.2 /usr/local/cuda\n",
        "!nvcc -g -G scale.cu -o scale"
      ],
      "metadata": {
        "id": "Dz4pCdMh_T2c"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will generate `scale` binary in the current directly. You can run this binary and see the output"
      ],
      "metadata": {
        "id": "Ljc4sPrXjCCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./scale"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzkXoHKF_kgC",
        "outputId": "7b1b1d83-5764-4653-f075-1fede89bed98"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 : 0\n",
            "1 : 5\n",
            "2 : 10\n",
            "3 : 15\n",
            "4 : 20\n",
            "5 : 25\n",
            "6 : 30\n",
            "7 : 35\n",
            "8 : 40\n",
            "9 : 45\n",
            "10 : 50\n",
            "11 : 55\n",
            "12 : 60\n",
            "13 : 65\n",
            "14 : 70\n",
            "15 : 75\n",
            "16 : 80\n",
            "17 : 85\n",
            "18 : 90\n",
            "19 : 95\n",
            "20 : 100\n",
            "21 : 105\n",
            "22 : 110\n",
            "23 : 115\n",
            "24 : 120\n",
            "25 : 125\n",
            "26 : 130\n",
            "27 : 135\n",
            "28 : 140\n",
            "29 : 145\n",
            "30 : 150\n",
            "31 : 155\n",
            "32 : 160\n",
            "33 : 165\n",
            "34 : 170\n",
            "35 : 175\n",
            "36 : 180\n",
            "37 : 185\n",
            "38 : 190\n",
            "39 : 195\n",
            "40 : 200\n",
            "41 : 205\n",
            "42 : 210\n",
            "43 : 215\n",
            "44 : 220\n",
            "45 : 225\n",
            "46 : 230\n",
            "47 : 235\n",
            "48 : 240\n",
            "49 : 245\n",
            "50 : 250\n",
            "51 : 255\n",
            "52 : 260\n",
            "53 : 265\n",
            "54 : 270\n",
            "55 : 275\n",
            "56 : 280\n",
            "57 : 285\n",
            "58 : 290\n",
            "59 : 295\n",
            "60 : 300\n",
            "61 : 305\n",
            "62 : 310\n",
            "63 : 315\n"
          ]
        }
      ]
    }
  ]
}